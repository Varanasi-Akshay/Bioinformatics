known_truth = train_data$am,
data_name = "training"
)
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = test_data$am,
data_name = "test"
)
df_combined <-rbind(df_train,df_test)
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
#xlim(0, 0.14) +
scale_color_colorblind()
# results data frame for training data
df_train <- data.frame(
predictor = predict(glm_model, train_data),
known_truth = factor(train_data$am),
data_name = "training"
)
# Density plot
ggplot(df_train, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = factor(test_data$am),
data_name = "test"
)
# Density plot
ggplot(df_test, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# Calculating Area under the curve for model on training and test data
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
# R code for your question goes here
train_data %>%
select(wt,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_train <- data.frame(pca$x, am = train_data$am)
head(pca_train)
# R code for your question goes here
test_data %>%
select(wt,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_data_test <- data.frame(pca$x, am = test_data$am)
head(pca_test)
library(knitr)
opts_chunk$set(fig.align = "center", fig.height = 4, fig.width = 5)
library(tidyverse)
theme_set(theme_bw(base_size = 12))
library(plotROC)
library(ggthemes)
head(mtcars)
# model to use:
# am ~ hp + mpg
# Making logistic regression model for the above model.
glm_model1 <-glm(
am ~ hp + mpg,
data = mtcars,
family = binomial
)
# Summary of the model 1
summary(glm_model1)
# model to use:
# am ~ hp
# Making logistic regression model for the above model.
glm_model2 <-glm(
am ~ hp,
data = mtcars,
family = binomial
)
# Summary of the model 2
summary(glm_model2)
# results data frame for model 1
df_model1 <- data.frame(
predictor = predict(glm_model1, mtcars),
known_truth = mtcars$am,
data_name = "Model 1"
)
# results data frame for model 2
df_model2 <- data.frame(
predictor = predict(glm_model2, mtcars),
known_truth = mtcars$am,
data_name = "Model 2"
)
# Combining both the dataframes to plot ROC curves
df_combined <-rbind(df_model1,df_model2)
# Plotting the ROC curve for both the Models
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
scale_color_colorblind()
# Calculating Area under the curve for models
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
train_fraction <- 0.5 # fraction of data for training purposes
set.seed(123) # set the seed to make the partition reproductible
train_size <- floor(train_fraction * nrow(mtcars)) # number of observations in training set
train_indices <- sample(1:nrow(mtcars), size = train_size)
train_data <- mtcars[train_indices, ] # get training data
test_data <- mtcars[-train_indices, ] # get test data
# model to use:
# am ~ hp + mpg
# Making logistic regression model for the above model. (This time we use training data only)
glm_model <-glm(
am ~ hp + mpg,
data = train_data, # Here we give only the training data instead of whole dataset
family = binomial
)
# results data frame for training data
df_train <- data.frame(
predictor = predict(glm_model, train_data),
known_truth = train_data$am,
data_name = "training"
)
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = test_data$am,
data_name = "test"
)
df_combined <-rbind(df_train,df_test)
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
#xlim(0, 0.14) +
scale_color_colorblind()
# results data frame for training data
df_train <- data.frame(
predictor = predict(glm_model, train_data),
known_truth = factor(train_data$am),
data_name = "training"
)
# Density plot
ggplot(df_train, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = factor(test_data$am),
data_name = "test"
)
# Density plot
ggplot(df_test, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# Calculating Area under the curve for model on training and test data
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
# R code for your question goes here
train_data %>%
select(wt,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_train <- data.frame(pca$x, am = train_data$am)
head(pca_train)
# R code for your question goes here
test_data %>%
select(wt,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_test <- data.frame(pca$x, am = test_data$am)
head(pca_test)
# am ~ hp + mpg + disp
# Making logistic regression model for the above model using original data.
glm_model1 <-glm(
am ~ wt + mpg + disp ,
data = train_data,
family = binomial
)
# Summary of the model 1
summary(glm_model1)
# am ~ pc1 + pc2 + pc3
# Making logistic regression model for the above model using pca data
glm_model2 <-glm(
am ~ PC1 + PC2 + PC3,
data = pca_train,
family = binomial
)
# Summary of the model 2
summary(glm_model2)
# results data frame for original data
df_original <- data.frame(
predictor = predict(glm_model1, test_data),
known_truth = test_data$am,
data_name = "original"
)
# results data frame for pca data
df_pca <- data.frame(
predictor = predict(glm_model2, pca_test),
known_truth = pca_data$am,
data_name = "pca"
)
df_combined <-rbind(df_original,df_pca)
# ROC curve of both
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
scale_color_colorblind()
# Calculating Area under the curve for model based on original and pca data
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
library(knitr)
opts_chunk$set(fig.align = "center", fig.height = 4, fig.width = 5)
library(tidyverse)
theme_set(theme_bw(base_size = 12))
library(plotROC)
library(ggthemes)
head(mtcars)
# model to use:
# am ~ hp + mpg
# Making logistic regression model for the above model.
glm_model1 <-glm(
am ~ hp + mpg,
data = mtcars,
family = binomial
)
# Summary of the model 1
summary(glm_model1)
# model to use:
# am ~ hp
# Making logistic regression model for the above model.
glm_model2 <-glm(
am ~ hp,
data = mtcars,
family = binomial
)
# Summary of the model 2
summary(glm_model2)
# results data frame for model 1
df_model1 <- data.frame(
predictor = predict(glm_model1, mtcars),
known_truth = mtcars$am,
data_name = "Model 1"
)
# results data frame for model 2
df_model2 <- data.frame(
predictor = predict(glm_model2, mtcars),
known_truth = mtcars$am,
data_name = "Model 2"
)
# Combining both the dataframes to plot ROC curves
df_combined <-rbind(df_model1,df_model2)
# Plotting the ROC curve for both the Models
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
scale_color_colorblind()
# Calculating Area under the curve for models
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
train_fraction <- 0.5 # fraction of data for training purposes
set.seed(123) # set the seed to make the partition reproductible
train_size <- floor(train_fraction * nrow(mtcars)) # number of observations in training set
train_indices <- sample(1:nrow(mtcars), size = train_size)
train_data <- mtcars[train_indices, ] # get training data
test_data <- mtcars[-train_indices, ] # get test data
# model to use:
# am ~ hp + mpg
# Making logistic regression model for the above model. (This time we use training data only)
glm_model <-glm(
am ~ hp + mpg,
data = train_data, # Here we give only the training data instead of whole dataset
family = binomial
)
# results data frame for training data
df_train <- data.frame(
predictor = predict(glm_model, train_data),
known_truth = train_data$am,
data_name = "training"
)
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = test_data$am,
data_name = "test"
)
df_combined <-rbind(df_train,df_test)
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
#xlim(0, 0.14) +
scale_color_colorblind()
# results data frame for training data
df_train <- data.frame(
predictor = predict(glm_model, train_data),
known_truth = factor(train_data$am),
data_name = "training"
)
# Density plot
ggplot(df_train, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# results data frame for testing data
df_test <- data.frame(
predictor = predict(glm_model, test_data),
known_truth = factor(test_data$am),
data_name = "test"
)
# Density plot
ggplot(df_test, aes(x = predictor, fill = known_truth )) +
geom_density(alpha = .5) +scale_fill_colorblind()
# Calculating Area under the curve for model on training and test data
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
# R code for your question goes here
train_data %>%
select(hp,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_train <- data.frame(pca$x, am = train_data$am)
head(pca_train)
# R code for your question goes here
test_data %>%
select(hp,mpg,disp) %>%   # remove am column
scale() %>%            # scale to 0 mean and unit variance
prcomp() ->            # do PCA
pca                    # store result as `pca`
# now display the results from the PCA analysis
pca_test <- data.frame(pca$x, am = test_data$am)
head(pca_test)
# am ~ hp + mpg + disp
# Making logistic regression model for the above model using original data.
glm_model1 <-glm(
am ~ hp + mpg + disp ,
data = train_data,
family = binomial
)
# Summary of the model 1
summary(glm_model1)
# am ~ pc1 + pc2 + pc3
# Making logistic regression model for the above model using pca data
glm_model2 <-glm(
am ~ PC1 + PC2 + PC3,
data = pca_train,
family = binomial
)
# Summary of the model 2
summary(glm_model2)
# Comparing model on training data
# results data frame for original data
df_original <- data.frame(
predictor = predict(glm_model1, train_data),
known_truth = train_data$am,
data_name = "original"
)
# results data frame for pca data
df_pca <- data.frame(
predictor = predict(glm_model2, pca_train),
known_truth = pca_train$am,
data_name = "pca"
)
df_combined <-rbind(df_original,df_pca)
# ROC curve of both
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
scale_color_colorblind()
# results data frame for original data
df_original <- data.frame(
predictor = predict(glm_model1, test_data),
known_truth = test_data$am,
data_name = "original"
)
# results data frame for pca data
df_pca <- data.frame(
predictor = predict(glm_model2, pca_test),
known_truth = pca_test$am,
data_name = "pca"
)
df_combined <-rbind(df_original,df_pca)
# ROC curve of both
ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0) +
scale_color_colorblind()
# Calculating Area under the curve for model based on original and pca data
p<-ggplot(df_combined, aes(d = known_truth, m = predictor, color = data_name)) +
geom_roc(n.cuts = 0)
data_name <- unique(df_combined$data_name)
data_name
data_info <- data.frame(
data_name,
group = order(data_name)
)
left_join(data_info, calc_auc(p)) %>%
select(-group, -PANEL) %>%
arrange(desc(AUC))
install.packages(dplyr)
install.packages('dplyr')
library(knitr)
opts_chunk$set(fig.align="center", fig.height=4.5, fig.width=6)
library(tidyverse)
install.packages('tidyverse')
library(tidyverse)
theme_set(theme_bw(base_size=12)) # set default ggplot2 theme
library(sf) # needed for simple feature manipulation
install.packages('sf')
library(tidyverse)
theme_set(theme_bw(base_size=12)) # set default ggplot2 theme
library(sf) # needed for simple feature manipulation
# load all data
load(url("https://wilkelab.org/classes/SDS348/data_sets/US_income.RData"))
# income and population data by state
head(US_income)
# income and population data by county
head(US_counties_income)
ggplot(US_income) +
geom_sf()
ggplot(US_income, aes(fill = log(population))) +
geom_sf(color = "black", size = 0.2) # draw state boundaries with thin black lines
# remove Alaska and Hawaii
lower48 <- US_income %>%
filter(!GEOID %in% c("02", "15"))
# plot
ggplot(lower48) + geom_sf()
ggplot(lower48) +
geom_sf() +
coord_sf(crs = 3395) # World Mercator, not recommended in practice, https://spatialreference.org/ref/epsg/3395/
ggplot(lower48) +
geom_sf() +
coord_sf(crs = 3338) # Normally used for Alaska, https://spatialreference.org/ref/epsg/nad83-alaska-albers/
# your R code goes here.
ggplot(lower48) +
geom_sf() +
coord_sf(crs = 4326)
# your R code goes here.
ggplot(US_counties_income) +
geom_sf() +
coord_sf(crs = 4326)
# your R code goes here.
ggplot(US_counties_income) +
geom_sf() +
#coord_sf(crs = 4326)
# your R code goes here.
ggplot(US_counties_income) +
geom_sf() +
#coord_sf(crs = 4326)
# your R code goes here.
ggplot(US_counties_income) +
geom_sf() + scale_fill_viridis_c(option = "B")
#coord_sf(crs = 4326)
# your R code goes here.
ggplot(US_counties_income,aes(fill = median(income))) +
geom_sf() + scale_fill_viridis_c(option = "B")
View(US_counties_income)
# your R code goes here.
ggplot(US_counties_income,aes(fill = median_income)) +
geom_sf() + scale_fill_viridis_c(option = "B")
#coord_sf(crs = 4326)
# your R code goes here.
US_counties_income%>%filter(STATEFP=='48')->texas_income
ggplot(texas_income,aes(fill = median_income)) +
geom_sf() + scale_fill_viridis_c(option = "B")
